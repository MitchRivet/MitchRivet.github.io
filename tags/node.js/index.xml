<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mitch Rivet</title>
    <link>http://mitchrivet.github.io/tags/node.js/index.xml</link>
    <description>Recent content on Mitch Rivet</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://mitchrivet.github.io/tags/node.js/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Analyzing Reddit Comments: Scraping (Part 1)</title>
      <link>http://mitchrivet.github.io/analyzing-reddit-comments-scraping-part-1</link>
      <pubDate>Sun, 06 Nov 2016 22:17:00 +0200</pubDate>
      
      <guid>http://mitchrivet.github.io/analyzing-reddit-comments-scraping-part-1</guid>
      <description>

&lt;p&gt;&lt;br&gt;
We are in an age where internet discourse has a serious impact on virtually every aspect of our society, making the analysis of user comments in a site like reddit a tantalizing data set (for better and worse). In this tutorial, I will go over:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scraping reddit&lt;/li&gt;
&lt;li&gt;Setting up a basic Node.js/ExpressJS server&lt;/li&gt;
&lt;li&gt;Using ES6 generators to help parse a set of nested comments and give comments a basic sentiment score&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/MitchRivet/reddit_scrape_tutorial&#34;&gt;Final code found in this repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-to-scrape-reddit&#34;&gt;How to scrape reddit&lt;/h2&gt;

&lt;p&gt;Two fetch this comments from Reddit, you essentially have two options. The first is using OAUTH to actually log into the Reddit API, with the downside being that you are limited to 60 requests per hour, making it hard to download an  thread and all of the relevant user meta data (popular discussions easily exceed 1,000 comments). You can read the Reddit API documentation &lt;a href=&#34;https://www.reddit.com/dev/api/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The option I will cover here is simply scraping a comment thread, which is essentially using a server to make HTTP requests for the pages you&amp;rsquo;re interested in. Reddit makes this process very nice by exposing their endpoints as JSON strings, which you can see by going to any thread on reddit and appending a &amp;lsquo;/.json&amp;rsquo; to the end of the url. You can then parse the JSON string and begin to analyze the data. The downside of this method is that you can end up with incomplete data sets, since comments are often nested inside a &amp;lsquo;load more comments&amp;rsquo; link for the sites readability. As you will see, the JSON object returned to you will contain objects that have a property called &amp;lsquo;body&amp;rsquo; with the value &amp;lsquo;load more comments&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;There are many different languages/frameworks one could use for this task. For this tutorial, I&amp;rsquo;m going to use Node.js to create an API that will communicate with a client/web-app that will allow users to paste links to threads they want scraped and generate data visualizations. If you know Python, another option for doing this is using &lt;a href=&#34;https://praw.readthedocs.io/en/stable/&#34;&gt;PRAW&lt;/a&gt; (I may write another tutorial with PRAW soon).&lt;/p&gt;

&lt;p&gt;First of all, make sure you have &lt;a href=&#34;https://nodejs.org/en/download/&#34;&gt;Node.js installed&lt;/a&gt;. To begin, open your terminal, make a directory for your project, navigate inside that directory and on the command line type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm init&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will prompt you through some questions about your project and generate a package.json file that you will use to manage your dependencies, which you can now install in the same directory with npm (Node Package Manager):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install express request sentiment&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that you now have a node_modules folder containing the scripts for these libraries. Express is a minimal web-framework for Node.js that will make our routing very easy. Request provides extremely easy HTTP calls. Sentiment will be used for scoring the positive/negative sentiment of the comments you find. Note that sentiment is actually an oversimplified way of doing this type of positive/negative analysis but will be fine for us to with. In the future, I will write about other options for natural language processing.&lt;/p&gt;

&lt;p&gt;Create a file called &amp;lsquo;server.js&amp;rsquo;. You can now use the following code to create your server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
    //server.js

    //declare our initial dependencies
    var express = require(&#39;express&#39;);
    var request = require(&#39;request&#39;);
    var sentiment = require(&#39;sentiment&#39;);

    //initialize express
    var app = express();

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        res.json({message: &#39;this is your endpoint&#39;});
    });

    app.listen(&#39;8081&#39;);     

    console.log(&#39;Listening on port 8081&#39;);
    exports = module.exports = app;

&lt;/pre&gt;&lt;/code&gt;


Save the file and start the server by running this command in your project directory:

&lt;pre&gt;&lt;code&gt;node server.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks to the console.log, you should get a message printed in the terminal saying &amp;lsquo;Listening on port 8081&amp;rsquo;. If everything is working correctly, you should be able to open our your browser to localhost:8081/scrape and see the JSON object with &amp;lsquo;this is your endpoint&amp;rsquo;. You now have an endpoint that returns data that could eventually be used by a client.&lt;/p&gt;

&lt;h2 id=&#34;making-requests&#34;&gt;Making Requests&lt;/h2&gt;

&lt;p&gt;Now lets use the Request module to download the content from Reddit. Modify your &amp;lsquo;/scrape&amp;rsquo; endpoint with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;//server.js
...
    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        //pick the url of whatever thread you are interested in
        var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

        request(url, (err, response, body) =&gt; {
            if (err) {
                console.log(err);
            } else {
                let commentJSON = JSON.parse(response.body);
                res.json(commentJSON);
            }
            });
    });
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, reload the server with the &amp;lsquo;node server.js&amp;rsquo; command, make the same request to localhost:8081/scrape in your browser. You should see a giant mess of JSON that contains whatever thread you wanted to look at. I recommend using &lt;a href=&#34;https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa?hl=en&#34;&gt;JSON Formatter&lt;/a&gt; in Chrome to make this data much more readable.&lt;/p&gt;

&lt;h2 id=&#34;parsing-comments&#34;&gt;Parsing Comments&lt;/h2&gt;

&lt;p&gt;Notice that the structure of the comments is nested in the same way they are visible on the html page. What if you want to analyze each comment individually? You would want to place each comment into a flat array. If you want to get a structure like this you will probably want to use some form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursion&#34;&gt;Recursion&lt;/a&gt;. Essentially, you would want to tell your program to go through each comment and ask &amp;ldquo;does this comment have any replies? if yes, then run the same operation on the array of comments inside of the reply property. If not, then continue processing the top level of the comments array&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;For this task, I decided to use a new syntax added to ES6 called a generator. &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*&#34;&gt;Generators&lt;/a&gt; are functions that can be exited and later re-entered and provide a nice syntax for the type of recursion you may want to do. To see a generator in action, add the following function globally, above your &amp;lsquo;/scrape&amp;rsquo; endpoint.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
    //
    //
    //pass in all of the returned comments
    function *findReplies(comments) {
        if (!comment) {return ;}

        //looping through all of the comments
        for (var i = 0; i &lt; comments.length; i++) {

            //which ever step of the array we are on, grab the data
            var commentMeta = comments[i].data;

            //checking that the comment was not deleted    
            if (commentMeta.body &amp;&amp; commentMeta.author !== &#34;[deleted]&#34;) {

              allWordsString = allWordsString + commentMeta.body + &#39;&#39;;
              var score = sentiment(commentMeta.body);

              yield {
                author: commentMeta.author,
                body: commentMeta.body,
                score: score
              };
            }

            if (commentMeta.replies) {
              yield *findReplies(commentMeta.replies.data.children);
            }
          }
    }

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
...&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;Every time this function hits a yield statement in this function, whatever it yields will be part of a larger return. This happens in two places: the first when the comment meta data is grabbed, and second when the generator is called on itself if the comment in question contains replies. This is also where I employ the sentiment library, which gives a basic positive/negative score for the comment in question and the most important words sentiment identified in the comment body.&lt;/p&gt;

&lt;p&gt;Now, how can this function be called?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;...
app.get(&#39;/scrape&#39;, (req, res) =&gt; {

    var flatComments = [];

    var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

    request(url, (err, response, html) =&gt; {
        if (err) {
            console.log(err);
        } else {
            let commentJSON = JSON.parse(response.body);
            let header = response.body[0];
            let comments = commentJSON[1].data.children;

            var getReply = findReplies(comments);
            var replyGrab = getReply.next();

            //the generator has a state that you can manage inside of a while statement
            while (!replyGrab.done) {
              flatComments.push(replyGrab.value);
              replyGrab = getReply.next();
            }

            //once the generator is finished, you can send our parsed data back to the client
            if (replyGrab.done) {
                res.json({title: url, comments: flatComments });
            }
        }
        });
});
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, open your browser again and double check that the data is being sent back. From here, you can write this data to file, save it to a database, or whatever else you like. To view the final code, visit &lt;a href=&#34;https://github.com/MitchRivet/reddit_scrape_tutorial&#34;&gt;my github repository&lt;/a&gt;. In next installments, I will take a lot at asynchronously fetching all of the user comments inside a thread, other options for language processing, and creating a ReactJS client to visualize data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>